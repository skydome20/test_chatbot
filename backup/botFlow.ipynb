{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Dumping model to file cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 5.612 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-09-06 22:23:08,205 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-09-06 22:23:08,222 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 5.337 seconds.\n",
      "2017-09-06 22:23:13,559 : DEBUG : Loading model cost 5.337 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-09-06 22:23:13,570 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "### 1. initial setup (import package)###\n",
    "\n",
    "# apply word2vec analysis\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# support package\n",
    "import time\n",
    "import better_exceptions\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "# self-package\n",
    "import question_module\n",
    "import news_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 2. initial setup (basic setup)###\n",
    "\n",
    "# prepare wiki word2vec matrix\n",
    "path = \"/home/skydome20/Desktop/dbot/dbot.py3/wiki_word2vec_set/wiki_article_word2vector_200_skipgram5_model.bin\"\n",
    "wiki_model = models.Word2Vec.load_word2vec_format(path, binary=True)\n",
    "\n",
    "# give question #\n",
    "question = \"金正恩和川普\"\n",
    "q_keywords = question_module.handler(question)\n",
    "print(q_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 3. parse related articles by given question ###\n",
    "\"\"\" \n",
    "news_bigTable_path = news_module.news_parse(q_keywords, \"+\")\n",
    "\n",
    "# the reference path\n",
    "title_path = news_bigTable_path[0] \n",
    "news_path = news_bigTable_path[1]  \n",
    "href_path = news_bigTable_path[2] \n",
    "\"\"\"\n",
    "title_path = \"/home/skydome20/Desktop/news_folder/title/台大_論文造假_title.txt\"\n",
    "news_path = \"/home/skydome20/Desktop/news_folder/all/台大_論文造假.txt\"\n",
    "href_path = \"/home/skydome20/Desktop/news_folder/href/台大_論文造假_href.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 4. Cut articles ###\n",
    "cut_news_path = news_module.news_cut(news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 04:52:39,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-04-06 04:52:39,162 : INFO : built Dictionary(8125 unique tokens: ['日前', '化所前', '視', '第二次', '續任']...) from 178 documents (total 50825 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original news: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 04:52:39,271 : INFO : collecting document frequencies\n",
      "2017-04-06 04:52:39,283 : INFO : PROGRESS: processing document #0\n",
      "2017-04-06 04:52:39,300 : INFO : calculating IDF weights for 178 documents and 8124 features (34217 matrix non-zeros)\n",
      "2017-04-06 04:52:39,327 : INFO : using serial LSI version on this node\n",
      "2017-04-06 04:52:39,334 : INFO : updating model with new documents\n",
      "2017-04-06 04:52:39,450 : INFO : preparing a new chunk of documents\n",
      "2017-04-06 04:52:39,475 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-04-06 04:52:39,483 : INFO : 1st phase: constructing (8125, 110) action matrix\n",
      "2017-04-06 04:52:39,508 : INFO : orthonormalizing (8125, 110) action matrix\n",
      "2017-04-06 04:52:39,928 : INFO : 2nd phase: running dense svd on (110, 178) matrix\n",
      "2017-04-06 04:52:40,011 : INFO : computing the final decomposition\n",
      "2017-04-06 04:52:40,017 : INFO : keeping 10 factors (discarding 75.572% of energy spectrum)\n",
      "2017-04-06 04:52:40,025 : INFO : processed documents up to #178\n",
      "2017-04-06 04:52:40,037 : INFO : topic #0(3.029): -0.134*\"楊泮池\" + -0.115*\"篇\" + -0.113*\"張正\" + -0.109*\"琪\" + -0.108*\"高醫大\" + -0.092*\"作者\" + -0.090*\"調查\" + -0.089*\"科技部\" + -0.082*\"嚴孟祿\" + -0.081*\"他\"\n",
      "2017-04-06 04:52:40,040 : INFO : topic #1(2.019): -0.359*\"高醫大\" + 0.303*\"張正\" + 0.297*\"琪\" + -0.229*\"辭呈\" + 0.182*\"她\" + -0.136*\"書面\" + 0.120*\"解聘\" + -0.108*\"借調\" + -0.097*\"嚴孟祿\" + -0.093*\"請辭\"\n",
      "2017-04-06 04:52:40,054 : INFO : topic #2(1.943): -0.359*\"高醫大\" + -0.289*\"張正\" + -0.283*\"琪\" + -0.171*\"辭呈\" + -0.168*\"她\" + -0.128*\"書面\" + 0.105*\"論壇\" + 0.104*\"教改\" + -0.103*\"解聘\" + 0.094*\"嚴孟祿\"\n",
      "2017-04-06 04:52:40,055 : INFO : topic #3(1.844): -0.293*\"嚴孟祿\" + 0.186*\"高醫大\" + -0.153*\"政風\" + 0.126*\"論壇\" + 0.125*\"教改\" + -0.120*\"借款\" + -0.112*\"張正\" + -0.110*\"琪\" + -0.109*\"款項\" + -0.104*\"對價\"\n",
      "2017-04-06 04:52:40,056 : INFO : topic #4(1.629): 0.162*\"廖俊智\" + -0.154*\"教改\" + -0.153*\"論壇\" + -0.145*\"高醫大\" + -0.114*\"政風\" + 0.110*\"楊泮池\" + 0.104*\"篇\" + -0.099*\"許\" + 0.098*\"中研院\" + -0.098*\"信服\"\n",
      "2017-04-06 04:52:40,064 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-04-06 04:52:40,093 : INFO : creating matrix with 178 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keep news: 23\n",
      "The keep news = [0, 2, 5, 6, 7, 8, 9, 10, 11, 14, 15, 18, 21, 27, 31, 33, 40, 44, 45, 49, 51, 85, 111]\n"
     ]
    }
   ],
   "source": [
    "### 5. remove high similarity articles ### \n",
    "filter_news_path = news_module.news_similar_filter(cut_news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :178\n",
      "[('學術', 67), ('倫理', 46), ('調查', 39), ('研究', 38), ('造假', 29), ('台灣', 26), ('校長', 24), ('楊泮池', 22), ('科技部', 22), ('教授', 22), ('教育部', 19), ('論文', 17), ('作者', 17), ('掛名', 15)]\n"
     ]
    }
   ],
   "source": [
    "### 6. associate keywords ### \n",
    "ass_keywords = news_module.news_associated_keywords(q_keywords, filter_news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['學術', '倫理', '調查', '研究', '造假']\n"
     ]
    }
   ],
   "source": [
    "print(ass_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 13:41:37,758 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['學術', '倫理', '調查', '研究', '造假']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 13:43:42,842 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:45:34,067 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:47:31,517 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:49:32,446 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:51:29,816 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:53:26,990 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:55:15,169 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:57:04,751 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-04-06 13:59:12,557 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n"
     ]
    }
   ],
   "source": [
    "### 7. deduction on associated_keywords  ###\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "deduction_words = []\n",
    "\n",
    "# check keywords in wiki_word2vec\n",
    "for word in ass_keywords:\n",
    "    try:\n",
    "        wiki_model[word]\n",
    "    except Exception as e:\n",
    "        ass_keywords.remove(word)\n",
    "\n",
    "print(ass_keywords)\n",
    "\n",
    "# use combination of ass_keywords to apply deduction on keywords\n",
    "for tuple_words in combinations(ass_keywords, 2):\n",
    "    \n",
    "    # word2vec addition result to deduction \n",
    "    combin_vec = wiki_model[tuple_words[0]] + wiki_model[tuple_words[1]]\n",
    "\n",
    "    # cosine similarity for most related words for word2vec addition result\n",
    "    lst = list()\n",
    "    for voc in wiki_model.vocab:\n",
    "        voc_vec = wiki_model[voc]\n",
    "        cos_sim = cosine_similarity(combin_vec, voc_vec)\n",
    "        lst.append( (voc, float(cos_sim)) )\n",
    "        \n",
    "    # top k most words \n",
    "    result = sorted(lst, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # store top k most words\n",
    "    deduction_words.extend([tup_result[0] for tup_result in result[2:10]])\n",
    "    \n",
    "#print (deduction_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['商週', '山西疫苗', '論文造假', '媒體藝術史', '應用倫理學']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 8. top 5 deduction words ###\n",
    "[word[0] for word in Counter(deduction_words).most_common(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[356, 9, 2]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
