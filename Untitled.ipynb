{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 1.506 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-02-22 14:08:51,284 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-02-22 14:08:51,293 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.221 seconds.\n",
      "2017-02-22 14:08:53,514 : DEBUG : Loading model cost 2.221 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-02-22 14:08:53,523 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= No filter ===============\n",
      "1st keywords:['客運', '致死']\n",
      "2nd keywords:52\n",
      "[('客運', 41), ('業者', 28), ('票價', 21), ('駕駛', 15), ('遊覽車', 14), ('台北', 12), ('調漲', 12), ('司機', 10), ('宜蘭', 8), ('檢查', 8)]\n",
      "['業者', '票價', '駕駛', '遊覽車', '台北']\n",
      "3rd keywords:147\n",
      "[('遊覽車', 103), ('駕駛', 79), ('司機', 43), ('台北', 35), ('業者', 24), ('行程', 23), ('台灣', 20), ('旅遊', 18), ('工時', 16), ('表示', 14)]\n",
      "['司機', '行程', '台灣', '旅遊', '工時']\n",
      "4nd keywords:176\n",
      "[('台灣', 133), ('司機', 56), ('遊覽車', 47), ('駕駛', 44), ('行程', 26), ('旅遊', 25), ('工時', 24), ('小時', 15), ('日本', 12), ('時間', 10)]\n",
      "['小時', '日本', '時間']\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.615 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-02-23 10:53:51,319 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-02-23 10:53:51,326 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.439 seconds.\n",
      "2017-02-23 10:53:53,765 : DEBUG : Loading model cost 2.439 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-02-23 10:53:53,785 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Similar filter ===============\n",
      "1st keywords:['客運', '致死']\n",
      "2nd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 11:05:40,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-23 11:05:40,581 : INFO : built Dictionary(3483 unique tokens: ['不支', '列為', '左側', '轉乘', '辭世']...) from 44 documents (total 12609 corpus positions)\n",
      "2017-02-23 11:05:40,619 : INFO : collecting document frequencies\n",
      "2017-02-23 11:05:40,626 : INFO : PROGRESS: processing document #0\n",
      "2017-02-23 11:05:40,628 : INFO : calculating IDF weights for 44 documents and 3482 features (7850 matrix non-zeros)\n",
      "2017-02-23 11:05:40,637 : INFO : using serial LSI version on this node\n",
      "2017-02-23 11:05:40,645 : INFO : updating model with new documents\n",
      "2017-02-23 11:05:40,667 : INFO : preparing a new chunk of documents\n",
      "2017-02-23 11:05:40,676 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-23 11:05:40,687 : INFO : 1st phase: constructing (3483, 110) action matrix\n",
      "2017-02-23 11:05:40,693 : INFO : orthonormalizing (3483, 110) action matrix\n",
      "2017-02-23 11:05:41,044 : INFO : 2nd phase: running dense svd on (110, 44) matrix\n",
      "2017-02-23 11:05:41,068 : INFO : computing the final decomposition\n",
      "2017-02-23 11:05:41,075 : INFO : keeping 10 factors (discarding 54.578% of energy spectrum)\n",
      "2017-02-23 11:05:41,080 : INFO : processed documents up to #44\n",
      "2017-02-23 11:05:41,088 : INFO : topic #0(2.059): 0.302*\"元\" + 0.258*\"票價\" + 0.224*\"0\" + 0.210*\"3月1日\" + 0.172*\"39\" + 0.164*\"台北\" + 0.161*\"調漲\" + 0.153*\"調整\" + 0.137*\"日統\" + 0.128*\"統聯\"\n",
      "2017-02-23 11:05:41,093 : INFO : topic #1(1.756): -0.327*\"勞工局\" + -0.189*\"檢查\" + -0.170*\"低價\" + -0.162*\"勞動\" + -0.157*\"遊覽車\" + -0.153*\"鄭素玲\" + -0.147*\"旅行團\" + -0.146*\"旅行社\" + -0.139*\"違法\" + -0.128*\"駕駛\"\n",
      "2017-02-23 11:05:41,094 : INFO : topic #2(1.494): -0.188*\"排班\" + 0.160*\"元\" + 0.148*\"39\" + -0.143*\"一休\" + -0.133*\"王國\" + -0.127*\"一例\" + -0.123*\"港口\" + -0.116*\"運價\" + 0.114*\"勞工局\" + -0.114*\"上路\"\n",
      "2017-02-23 11:05:41,098 : INFO : topic #3(1.405): -0.216*\"車車\" + -0.197*\"遊覽\" + -0.181*\"遊覽車\" + -0.163*\"這輛\" + -0.163*\"哪裡\" + -0.148*\"19年\" + -0.146*\"10年\" + -0.131*\"齡\" + -0.113*\"33\" + -0.111*\"死\"\n",
      "2017-02-23 11:05:41,102 : INFO : topic #4(1.348): 0.194*\"39\" + -0.173*\"優惠\" + 0.167*\"排班\" + 0.165*\"元\" + 0.130*\"日統\" + 0.127*\"王國\" + -0.124*\"票價\" + -0.119*\"新聞稿\" + 0.109*\"運價\" + 0.109*\"港口\"\n",
      "2017-02-23 11:05:41,103 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-23 11:05:41,117 : INFO : creating matrix with 44 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :44\n",
      "[('客運', 38), ('業者', 23), ('票價', 21), ('台北', 15), ('遊覽車', 15), ('司機', 13), ('調漲', 12), ('駕駛', 11), ('調整', 9), ('公車', 8)]\n",
      "['業者', '票價', '台北', '遊覽車', '司機']\n",
      "3rd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 11:16:52,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-23 11:16:52,798 : INFO : built Dictionary(11344 unique tokens: ['仍停留', '心態', '漲並', '李鴻鈞', '劣質']...) from 154 documents (total 57349 corpus positions)\n",
      "2017-02-23 11:16:52,938 : INFO : collecting document frequencies\n",
      "2017-02-23 11:16:52,943 : INFO : PROGRESS: processing document #0\n",
      "2017-02-23 11:16:52,964 : INFO : calculating IDF weights for 154 documents and 11343 features (37432 matrix non-zeros)\n",
      "2017-02-23 11:16:52,996 : INFO : using serial LSI version on this node\n",
      "2017-02-23 11:16:53,006 : INFO : updating model with new documents\n",
      "2017-02-23 11:16:53,156 : INFO : preparing a new chunk of documents\n",
      "2017-02-23 11:16:53,187 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-23 11:16:53,195 : INFO : 1st phase: constructing (11344, 110) action matrix\n",
      "2017-02-23 11:16:53,215 : INFO : orthonormalizing (11344, 110) action matrix\n",
      "2017-02-23 11:16:54,086 : INFO : 2nd phase: running dense svd on (110, 154) matrix\n",
      "2017-02-23 11:16:54,126 : INFO : computing the final decomposition\n",
      "2017-02-23 11:16:54,137 : INFO : keeping 10 factors (discarding 77.919% of energy spectrum)\n",
      "2017-02-23 11:16:54,147 : INFO : processed documents up to #154\n",
      "2017-02-23 11:16:54,152 : INFO : topic #0(2.636): 0.150*\"駕駛\" + 0.126*\"遊覽車\" + 0.116*\"行程\" + 0.116*\"司機\" + 0.112*\"休息\" + 0.112*\"工時\" + 0.100*\"小時\" + 0.097*\"交通部\" + 0.091*\"林美珠\" + 0.088*\"一日遊\"\n",
      "2017-02-23 11:16:54,159 : INFO : topic #1(1.862): 0.292*\"元\" + 0.287*\"調漲\" + 0.274*\"票價\" + 0.235*\"元漲\" + 0.225*\"漲幅\" + 0.191*\"漲價\" + 0.178*\"統聯\" + 0.162*\"漲\" + 0.147*\"客運\" + 0.147*\"台北\"\n",
      "2017-02-23 11:16:54,161 : INFO : topic #2(1.697): 0.224*\"一日遊\" + -0.207*\"林美珠\" + 0.183*\"行程\" + 0.157*\"雙\" + 0.154*\"旅遊\" + -0.134*\"雇主\" + 0.126*\"國旅\" + 0.126*\"商品\" + 0.125*\"燦星\" + 0.122*\"採取\"\n",
      "2017-02-23 11:16:54,162 : INFO : topic #3(1.654): 0.285*\"林美珠\" + 0.188*\"雇主\" + -0.163*\"友力\" + 0.160*\"拘束\" + -0.146*\"靠行\" + 0.133*\"休息\" + -0.133*\"鄭寶清\" + 0.122*\"工時\" + 0.107*\"脫離\" + -0.102*\"通運\"\n",
      "2017-02-23 11:16:54,167 : INFO : topic #4(1.531): 0.181*\"鄭寶清\" + 0.172*\"靠行\" + 0.161*\"友力\" + -0.132*\"安全帶\" + 0.117*\"生計\" + 0.105*\"處分\" + -0.104*\"煞車\" + -0.097*\"乘客\" + 0.091*\"林美珠\" + 0.087*\"交通部\"\n",
      "2017-02-23 11:16:54,169 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-23 11:16:54,195 : INFO : creating matrix with 154 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :154\n",
      "[('遊覽車', 110), ('司機', 86), ('駕駛', 53), ('台北', 34), ('行程', 22), ('工時', 20), ('業者', 20), ('旅行社', 19), ('旅遊', 17), ('台灣', 15)]\n",
      "['駕駛', '行程', '工時', '旅行社', '旅遊']\n",
      "4nd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 11:27:26,170 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-23 11:27:26,337 : INFO : built Dictionary(10211 unique tokens: ['仍停留', '桃竹', '人煙稀少', '變為', '傾']...) from 143 documents (total 49413 corpus positions)\n",
      "2017-02-23 11:27:26,455 : INFO : collecting document frequencies\n",
      "2017-02-23 11:27:26,461 : INFO : PROGRESS: processing document #0\n",
      "2017-02-23 11:27:26,483 : INFO : calculating IDF weights for 143 documents and 10210 features (32642 matrix non-zeros)\n",
      "2017-02-23 11:27:26,526 : INFO : using serial LSI version on this node\n",
      "2017-02-23 11:27:26,527 : INFO : updating model with new documents\n",
      "2017-02-23 11:27:26,640 : INFO : preparing a new chunk of documents\n",
      "2017-02-23 11:27:26,657 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-23 11:27:26,675 : INFO : 1st phase: constructing (10211, 110) action matrix\n",
      "2017-02-23 11:27:26,692 : INFO : orthonormalizing (10211, 110) action matrix\n",
      "2017-02-23 11:27:27,620 : INFO : 2nd phase: running dense svd on (110, 143) matrix\n",
      "2017-02-23 11:27:27,641 : INFO : computing the final decomposition\n",
      "2017-02-23 11:27:27,647 : INFO : keeping 10 factors (discarding 75.524% of energy spectrum)\n",
      "2017-02-23 11:27:27,653 : INFO : processed documents up to #143\n",
      "2017-02-23 11:27:27,661 : INFO : topic #0(2.766): -0.127*\"駕駛\" + -0.110*\"旅遊\" + -0.107*\"一日遊\" + -0.107*\"雙\" + -0.107*\"行程\" + -0.104*\"觀光局\" + -0.104*\"司機\" + -0.102*\"遊覽車\" + -0.095*\"工時\" + -0.095*\"業者\"\n",
      "2017-02-23 11:27:27,667 : INFO : topic #1(1.950): 0.135*\"林美珠\" + 0.129*\"工作時間\" + -0.126*\"一日遊\" + -0.111*\"燦星\" + 0.109*\"勞動部\" + 0.107*\"雇主\" + -0.104*\"國旅\" + -0.100*\"雙\" + -0.099*\"觀光局\" + -0.099*\"旅遊\"\n",
      "2017-02-23 11:27:27,669 : INFO : topic #2(1.855): -0.180*\"妥適\" + -0.175*\"檢視\" + -0.173*\"應\" + -0.140*\"應使\" + -0.131*\"大原\" + -0.130*\"原則\" + -0.126*\"距離\" + -0.122*\"公里\" + -0.122*\"之\" + -0.116*\"加計\"\n",
      "2017-02-23 11:27:27,670 : INFO : topic #3(1.725): -0.149*\"蘋果\" + -0.123*\"林全\" + -0.116*\"毆\" + -0.116*\"下月\" + -0.116*\"公分\" + -0.116*\"通車\" + -0.116*\"限水\" + -0.116*\"裂\" + -0.108*\"新北\" + -0.098*\"起\"\n",
      "2017-02-23 11:27:27,675 : INFO : topic #4(1.697): 0.203*\"投保\" + -0.156*\"工作時間\" + -0.141*\"勞動部\" + -0.139*\"林美珠\" + 0.130*\"周比蒼\" + 0.118*\"周比\" + -0.111*\"雇主\" + 0.110*\"周繼弘\" + 0.107*\"弟弟\" + -0.105*\"勞務\"\n",
      "2017-02-23 11:27:27,677 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-23 11:27:27,704 : INFO : creating matrix with 143 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :143\n",
      "[('駕駛', 99), ('遊覽車', 61), ('司機', 57), ('旅遊', 47), ('行程', 44), ('工時', 32), ('旅行社', 32), ('業者', 25), ('小時', 21), ('時間', 16)]\n",
      "['小時', '時間']\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 1.609 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-02-22 19:12:29,625 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-02-22 19:12:29,632 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.287 seconds.\n",
      "2017-02-22 19:12:31,920 : DEBUG : Loading model cost 2.287 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-02-22 19:12:31,927 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Similar filter ===============\n",
      "1st keywords:['女', '大生', '去', '麵包', '坊', '實習', '被', '索賠']\n",
      "2nd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-22 19:22:48,707 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-22 19:22:48,735 : INFO : built Dictionary(2630 unique tokens: ['見狀', '企業', '廚房', '外界', '看起來']...) from 15 documents (total 6069 corpus positions)\n",
      "2017-02-22 19:22:48,754 : INFO : collecting document frequencies\n",
      "2017-02-22 19:22:48,756 : INFO : PROGRESS: processing document #0\n",
      "2017-02-22 19:22:48,757 : INFO : calculating IDF weights for 15 documents and 2629 features (3883 matrix non-zeros)\n",
      "2017-02-22 19:22:48,763 : INFO : using serial LSI version on this node\n",
      "2017-02-22 19:22:48,765 : INFO : updating model with new documents\n",
      "2017-02-22 19:22:48,778 : INFO : preparing a new chunk of documents\n",
      "2017-02-22 19:22:48,780 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-22 19:22:48,782 : INFO : 1st phase: constructing (2630, 110) action matrix\n",
      "2017-02-22 19:22:48,786 : INFO : orthonormalizing (2630, 110) action matrix\n",
      "2017-02-22 19:22:49,054 : INFO : 2nd phase: running dense svd on (110, 15) matrix\n",
      "2017-02-22 19:22:49,064 : INFO : computing the final decomposition\n",
      "2017-02-22 19:22:49,071 : INFO : keeping 10 factors (discarding 20.817% of energy spectrum)\n",
      "2017-02-22 19:22:49,072 : INFO : processed documents up to #15\n",
      "2017-02-22 19:22:49,079 : INFO : topic #0(1.555): 0.271*\"工房\" + 0.262*\"核四\" + 0.217*\"封存\" + 0.208*\"員工\" + 0.160*\"核四廠\" + 0.155*\"預算\" + 0.138*\"億元\" + 0.138*\"麵包\" + 0.129*\"設備\" + 0.126*\"台電\"\n",
      "2017-02-22 19:22:49,083 : INFO : topic #1(1.233): 0.305*\"黃士\" + 0.272*\"福\" + 0.178*\"網友\" + 0.162*\"實習\" + 0.149*\"羅勒\" + 0.144*\"珦\" + 0.144*\"新南\" + 0.129*\"黃\" + 0.120*\"蛋糕\" + 0.111*\"手感\"\n",
      "2017-02-22 19:22:49,084 : INFO : topic #2(1.124): -0.325*\"實習\" + 0.194*\"黃士\" + 0.173*\"福\" + -0.171*\"香港\" + -0.169*\"學生\" + -0.161*\"MBA\" + 0.099*\"羅勒\" + -0.096*\"總部\" + -0.095*\"周世婷\" + -0.094*\"機會\"\n",
      "2017-02-22 19:22:49,090 : INFO : topic #3(1.038): 0.232*\"實習\" + -0.210*\"女孩\" + -0.162*\"小\" + -0.141*\"德國\" + -0.140*\"吃\" + 0.139*\"網友\" + -0.122*\"小女孩\" + 0.117*\"香港\" + -0.108*\"麵包\" + -0.105*\"手作\"\n",
      "2017-02-22 19:22:49,091 : INFO : topic #4(1.000): 0.185*\"女\" + 0.185*\"警方\" + 0.185*\"逮捕\" + 0.142*\"大生\" + 0.124*\"明路\" + 0.124*\"指南\" + 0.124*\"胡姓\" + 0.124*\"來電\" + 0.124*\"車手\" + 0.124*\"前往\"\n",
      "2017-02-22 19:22:49,098 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-22 19:22:49,103 : INFO : creating matrix with 15 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :15\n",
      "[('麵包', 10), ('工房', 8), ('員工', 7), ('實習', 7), ('黃士', 6), ('封存', 5), ('預算', 4), ('學生', 4), ('台灣', 3), ('核四', 3)]\n",
      "['工房', '員工', '黃士', '封存', '預算', '學生']\n",
      "3rd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-22 19:33:10,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-22 19:33:10,630 : INFO : built Dictionary(12058 unique tokens: ['菜鳥', '包包', '港中', '東區', '不過']...) from 141 documents (total 56528 corpus positions)\n",
      "2017-02-22 19:33:10,758 : INFO : collecting document frequencies\n",
      "2017-02-22 19:33:10,770 : INFO : PROGRESS: processing document #0\n",
      "2017-02-22 19:33:10,787 : INFO : calculating IDF weights for 141 documents and 12057 features (35080 matrix non-zeros)\n",
      "2017-02-22 19:33:10,820 : INFO : using serial LSI version on this node\n",
      "2017-02-22 19:33:10,823 : INFO : updating model with new documents\n",
      "2017-02-22 19:33:10,963 : INFO : preparing a new chunk of documents\n",
      "2017-02-22 19:33:10,989 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-22 19:33:11,001 : INFO : 1st phase: constructing (12058, 110) action matrix\n",
      "2017-02-22 19:33:11,021 : INFO : orthonormalizing (12058, 110) action matrix\n",
      "2017-02-22 19:33:11,918 : INFO : 2nd phase: running dense svd on (110, 141) matrix\n",
      "2017-02-22 19:33:11,996 : INFO : computing the final decomposition\n",
      "2017-02-22 19:33:12,006 : INFO : keeping 10 factors (discarding 76.803% of energy spectrum)\n",
      "2017-02-22 19:33:12,017 : INFO : processed documents up to #141\n",
      "2017-02-22 19:33:12,033 : INFO : topic #0(2.331): -0.193*\"離職\" + -0.187*\"員工\" + -0.181*\"台鐵\" + -0.147*\"業者\" + -0.144*\"求償\" + -0.129*\"學生\" + -0.127*\"坊\" + -0.111*\"烘焙\" + -0.098*\"春節\" + -0.096*\"老闆\"\n",
      "2017-02-22 19:33:12,042 : INFO : topic #1(2.047): 0.327*\"陽台\" + 0.245*\"金大\" + 0.237*\"蒿\" + 0.237*\"安琪\" + 0.224*\"禽流感\" + 0.180*\"鴿子\" + 0.156*\"金門\" + 0.156*\"干擾\" + 0.156*\"鳥類\" + 0.139*\"恐懼\"\n",
      "2017-02-22 19:33:12,043 : INFO : topic #2(1.921): 0.344*\"台鐵\" + -0.235*\"離職\" + 0.190*\"春節\" + -0.187*\"業者\" + -0.186*\"求償\" + -0.161*\"坊\" + 0.159*\"交通部\" + 0.159*\"休假\" + 0.148*\"曠職\" + 0.140*\"罷工\"\n",
      "2017-02-22 19:33:12,054 : INFO : topic #3(1.818): -0.255*\"台鐵\" + 0.135*\"國際\" + 0.128*\"學生\" + -0.123*\"交通部\" + -0.121*\"春節\" + -0.116*\"休假\" + 0.114*\"香港\" + 0.111*\"課程\" + -0.111*\"員工\" + -0.110*\"罷工\"\n",
      "2017-02-22 19:33:12,055 : INFO : topic #4(1.642): 0.325*\"周姓\" + 0.238*\"華岡藝校\" + 0.191*\"教育局\" + 0.181*\"侵害\" + 0.159*\"權利\" + 0.143*\"校方\" + 0.138*\"行政訴訟\" + 0.138*\"馮喬蘭\" + 0.138*\"學生會\" + 0.119*\"大過\"\n",
      "2017-02-22 19:33:12,058 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-22 19:33:12,099 : INFO : creating matrix with 141 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :141\n",
      "[('學生', 102), ('員工', 54), ('學校', 21), ('台鐵', 21), ('離職', 20), ('表示', 15), ('公司', 13), ('學習', 12), ('大學', 12), ('獎學金', 12)]\n",
      "['學校', '台鐵', '離職', '表示', '公司']\n",
      "4nd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-22 19:38:09,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-22 19:38:09,468 : INFO : built Dictionary(6145 unique tokens: ['被告', '派出', '本份', '本件', '不過']...) from 41 documents (total 20446 corpus positions)\n",
      "2017-02-22 19:38:09,501 : INFO : collecting document frequencies\n",
      "2017-02-22 19:38:09,507 : INFO : PROGRESS: processing document #0\n",
      "2017-02-22 19:38:09,510 : INFO : calculating IDF weights for 41 documents and 6144 features (12355 matrix non-zeros)\n",
      "2017-02-22 19:38:09,521 : INFO : using serial LSI version on this node\n",
      "2017-02-22 19:38:09,524 : INFO : updating model with new documents\n",
      "2017-02-22 19:38:09,578 : INFO : preparing a new chunk of documents\n",
      "2017-02-22 19:38:09,584 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-22 19:38:09,591 : INFO : 1st phase: constructing (6145, 110) action matrix\n",
      "2017-02-22 19:38:09,599 : INFO : orthonormalizing (6145, 110) action matrix\n",
      "2017-02-22 19:38:10,197 : INFO : 2nd phase: running dense svd on (110, 41) matrix\n",
      "2017-02-22 19:38:10,224 : INFO : computing the final decomposition\n",
      "2017-02-22 19:38:10,238 : INFO : keeping 10 factors (discarding 64.819% of energy spectrum)\n",
      "2017-02-22 19:38:10,241 : INFO : processed documents up to #41\n",
      "2017-02-22 19:38:10,248 : INFO : topic #0(1.835): 0.372*\"台鐵\" + 0.216*\"休假\" + 0.195*\"春節\" + 0.195*\"曠職\" + 0.160*\"產業工會\" + 0.158*\"台鐵局\" + 0.157*\"員工\" + 0.156*\"交通部\" + 0.123*\"罷工\" + 0.123*\"依法\"\n",
      "2017-02-22 19:38:10,254 : INFO : topic #1(1.376): -0.259*\"離職\" + -0.226*\"老闆\" + -0.204*\"學生\" + -0.185*\"我\" + -0.164*\"坊\" + 0.159*\"台鐵\" + -0.146*\"你\" + -0.145*\"烘焙\" + -0.137*\"萬元\" + -0.122*\"契約\"\n",
      "2017-02-22 19:38:10,257 : INFO : topic #2(1.223): -0.446*\"我\" + -0.146*\"矽谷\" + 0.144*\"學生\" + 0.130*\"坊\" + 0.129*\"離職\" + -0.129*\"公司\" + 0.115*\"烘焙\" + -0.110*\"自己\" + -0.103*\"這些\" + -0.103*\"...\"\n",
      "2017-02-22 19:38:10,262 : INFO : topic #3(1.104): 0.240*\"評鑑\" + 0.227*\"學校\" + -0.186*\"我\" + 0.118*\"華岡藝校\" + 0.116*\"教育部\" + 0.111*\"選舉\" + -0.100*\"離職\" + 0.100*\"農會\" + 0.092*\"延後\" + 0.087*\"到校\"\n",
      "2017-02-22 19:38:10,263 : INFO : topic #4(1.065): -0.227*\"學校\" + -0.217*\"評鑑\" + 0.151*\"遊覽車\" + 0.141*\"司機\" + 0.138*\"旅行社\" + 0.136*\"公路總局\" + 0.130*\"蝶戀花\" + -0.120*\"學生\" + -0.117*\"華岡藝校\" + 0.111*\"長\"\n",
      "2017-02-22 19:38:10,264 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-22 19:38:10,282 : INFO : creating matrix with 41 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news is :41\n",
      "[('台鐵', 25), ('員工', 16), ('休假', 16), ('公司', 16), ('表示', 12), ('曠職', 8), ('學校', 7), ('學生', 6), ('離職', 6), ('春節', 5)]\n",
      "['休假', '曠職', '春節']\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 1.677 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-02-23 01:14:53,521 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-02-23 01:14:53,529 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.159 seconds.\n",
      "2017-02-23 01:14:55,689 : DEBUG : Loading model cost 2.159 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-02-23 01:14:55,698 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= No filter ===============\n",
      "1st keywords:['女', '大生', '去', '麵包', '坊', '實習', '被', '索賠']\n",
      "2nd keywords:The number of news is :11\n",
      "[('麵包', 8), ('工房', 8), ('員工', 7), ('封存', 5), ('黃士', 4), ('預算', 4), ('蛋糕', 3), ('實習', 3), ('學生', 3), ('核四', 3)]\n",
      "['工房', '員工', '封存', '黃士', '預算']\n",
      "3rd keywords:The number of news is :128\n",
      "[('員工', 105), ('台鐵', 34), ('預算', 28), ('公司', 25), ('離職', 17), ('工作', 15), ('曠職', 15), ('休假', 13), ('學生', 11), ('主管', 9)]\n",
      "['台鐵', '公司', '離職', '工作', '曠職']\n",
      "4nd keywords:The number of news is :117\n",
      "[('台鐵', 68), ('員工', 47), ('工作', 44), ('公司', 42), ('離職', 26), ('休假', 25), ('曠職', 18), ('學生', 18), ('公司法', 13), ('求償', 11)]\n",
      "['休假', '學生', '公司法', '求償']\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 1.397 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-02-23 17:50:56,193 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-02-23 17:50:56,200 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.320 seconds.\n",
      "2017-02-23 17:50:58,520 : DEBUG : Loading model cost 2.320 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-02-23 17:50:58,528 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Similar filter ===============\n",
      "1st keywords:['Uber', '罰款', '議題']\n",
      "2nd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 18:02:44,092 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-23 18:02:44,217 : INFO : built Dictionary(9116 unique tokens: ['打點', '業別', '分子', '退意', '中午']...) from 121 documents (total 47290 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original news: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 18:02:44,332 : INFO : collecting document frequencies\n",
      "2017-02-23 18:02:44,338 : INFO : PROGRESS: processing document #0\n",
      "2017-02-23 18:02:44,360 : INFO : calculating IDF weights for 121 documents and 9115 features (28665 matrix non-zeros)\n",
      "2017-02-23 18:02:44,376 : INFO : using serial LSI version on this node\n",
      "2017-02-23 18:02:44,386 : INFO : updating model with new documents\n",
      "2017-02-23 18:02:44,501 : INFO : preparing a new chunk of documents\n",
      "2017-02-23 18:02:44,516 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-23 18:02:44,529 : INFO : 1st phase: constructing (9116, 110) action matrix\n",
      "2017-02-23 18:02:44,541 : INFO : orthonormalizing (9116, 110) action matrix\n",
      "2017-02-23 18:02:45,399 : INFO : 2nd phase: running dense svd on (110, 121) matrix\n",
      "2017-02-23 18:02:45,429 : INFO : computing the final decomposition\n",
      "2017-02-23 18:02:45,434 : INFO : keeping 10 factors (discarding 75.337% of energy spectrum)\n",
      "2017-02-23 18:02:45,444 : INFO : processed documents up to #121\n",
      "2017-02-23 18:02:45,448 : INFO : topic #0(2.562): -0.286*\"Uber\" + -0.167*\"司機\" + -0.125*\"交通部\" + -0.119*\"合作\" + -0.118*\"議題\" + -0.117*\"計程車\" + -0.114*\"台灣\" + -0.102*\"服務\" + -0.093*\"國是會議\" + -0.090*\"乘客\"\n",
      "2017-02-23 18:02:45,454 : INFO : topic #1(2.066): 0.246*\"議題\" + 0.244*\"國是會議\" + 0.238*\"司法\" + 0.208*\"委員\" + 0.200*\"檢察官\" + 0.197*\"司改\" + -0.176*\"Uber\" + 0.143*\"改革\" + 0.135*\"法官\" + 0.130*\"會議\"\n",
      "2017-02-23 18:02:45,455 : INFO : topic #2(1.728): -0.368*\"越南交通\" + -0.340*\"部\" + -0.316*\"Vietnam\" + -0.288*\"越南\" + -0.257*\"客運\" + -0.230*\"應用\" + -0.134*\"規定\" + -0.134*\"活動\" + -0.127*\"經營\" + -0.120*\"提案\"\n",
      "2017-02-23 18:02:45,456 : INFO : topic #3(1.647): 0.321*\"張仙\" + 0.185*\"平\" + 0.156*\"產業\" + 0.139*\"導入\" + 0.139*\"Taiwan\" + 0.131*\"營運模式\" + 0.130*\"評分\" + 0.120*\"運輸\" + -0.118*\"唐鳳\" + -0.107*\"合作\"\n",
      "2017-02-23 18:02:45,460 : INFO : topic #4(1.521): 0.269*\"唐鳳\" + 0.182*\"多元化\" + -0.169*\"熱門\" + 0.163*\"方案\" + -0.138*\"里程\" + -0.127*\"環繞\" + -0.127*\"地球\" + 0.123*\"退出\" + -0.119*\"330\" + -0.114*\"總\"\n",
      "2017-02-23 18:02:45,463 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-23 18:02:45,504 : INFO : creating matrix with 121 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keep news: 17\n",
      "The number of news is :17\n",
      "[('台灣', 10), ('uber', 9), ('產業', 6), ('服務', 5), ('議題', 4), ('車輛', 4), ('合作', 3), ('司機', 3), ('平台', 3), ('應用', 3)]\n",
      "['台灣', 'uber', '產業', '服務', '車輛']\n",
      "3rd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 18:07:08,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-23 18:07:08,263 : INFO : built Dictionary(8716 unique tokens: ['與民爭利', '依稀記得', '收進', '年訂', '見']...) from 54 documents (total 26529 corpus positions)\n",
      "2017-02-23 18:07:08,322 : INFO : collecting document frequencies\n",
      "2017-02-23 18:07:08,326 : INFO : PROGRESS: processing document #0\n",
      "2017-02-23 18:07:08,333 : INFO : calculating IDF weights for 54 documents and 8715 features (17177 matrix non-zeros)\n",
      "2017-02-23 18:07:08,357 : INFO : using serial LSI version on this node\n",
      "2017-02-23 18:07:08,367 : INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original news: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 18:07:08,429 : INFO : preparing a new chunk of documents\n",
      "2017-02-23 18:07:08,444 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-23 18:07:08,451 : INFO : 1st phase: constructing (8716, 110) action matrix\n",
      "2017-02-23 18:07:08,461 : INFO : orthonormalizing (8716, 110) action matrix\n",
      "2017-02-23 18:07:09,251 : INFO : 2nd phase: running dense svd on (110, 54) matrix\n",
      "2017-02-23 18:07:09,274 : INFO : computing the final decomposition\n",
      "2017-02-23 18:07:09,282 : INFO : keeping 10 factors (discarding 72.328% of energy spectrum)\n",
      "2017-02-23 18:07:09,285 : INFO : processed documents up to #54\n",
      "2017-02-23 18:07:09,289 : INFO : topic #0(1.503): 0.139*\"美國商會\" + 0.134*\"企業\" + 0.118*\"星巴克\" + 0.103*\"調查\" + 0.100*\"錦華\" + 0.098*\"司機\" + 0.094*\"蘋果\" + 0.091*\"不確定性\" + 0.089*\"邊緣化\" + 0.089*\"受訪\"\n",
      "2017-02-23 18:07:09,295 : INFO : topic #1(1.393): 0.164*\"蘋果\" + 0.143*\"毒品\" + -0.120*\"美國商會\" + 0.095*\"228\" + 0.095*\"小模\" + 0.095*\"北韓\" + 0.095*\"12年\" + -0.087*\"錦華\" + -0.086*\"調查\" + 0.084*\"9\"\n",
      "2017-02-23 18:07:09,296 : INFO : topic #2(1.336): 0.380*\"星巴克\" + -0.201*\"美國商會\" + -0.156*\"企業\" + -0.151*\"錦華\" + 0.148*\"李茂生\" + -0.135*\"調查\" + -0.134*\"不確定性\" + -0.131*\"受訪\" + 0.118*\"漲價\" + 0.111*\"錢\"\n",
      "2017-02-23 18:07:09,297 : INFO : topic #3(1.276): 0.473*\"星巴克\" + -0.215*\"紅藜\" + 0.185*\"李茂生\" + 0.148*\"漲價\" + 0.125*\"錢\" + 0.125*\"花\" + 0.119*\"面子\" + 0.115*\"臺灣人\" + -0.110*\"司機\" + 0.107*\"美國商會\"\n",
      "2017-02-23 18:07:09,302 : INFO : topic #4(1.221): -0.695*\"紅藜\" + -0.209*\"藜麥\" + -0.186*\"大腸癌\" + -0.110*\"糧食\" + -0.101*\"食用\" + 0.101*\"司機\" + -0.097*\"前期\" + -0.097*\"病變\" + -0.095*\"養生\" + -0.093*\"預防\"\n",
      "2017-02-23 18:07:09,304 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-02-23 18:07:09,318 : INFO : creating matrix with 54 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keep news: 20\n",
      "The number of news is :20\n",
      "[('台灣', 21), ('司機', 4), ('印度', 4), ('投資', 4), ('戲劇', 3), ('市場', 3), ('產業', 3), ('服務', 2), ('報告', 2), ('辦事處', 2)]\n",
      "['司機', '印度', '投資', '戲劇', '市場']\n",
      "4nd keywords:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-23 18:09:09,052 : INFO : built Dictionary(0 unique tokens: []) from 0 documents (total 0 corpus positions)\n",
      "2017-02-23 18:09:09,057 : INFO : collecting document frequencies\n",
      "2017-02-23 18:09:09,061 : INFO : calculating IDF weights for 0 documents and 0 features (0 matrix non-zeros)\n",
      "2017-02-23 18:09:09,066 : INFO : using serial LSI version on this node\n",
      "2017-02-23 18:09:09,068 : INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original news: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "decomposition not initialized yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/home/skydome20/Desktop/dbot/dbot.py3/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mcut_news_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_bigTable_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mfilter_news_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_similar_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_news_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_associated_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabo_keywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_news_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/skydome20/Desktop/dbot/dbot.py3/news_module.py\u001b[0m in \u001b[0;36mnews_similar_filter\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;31m# create index of LSI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# check similar docs ind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/skydome20/.local/lib/python3.5/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, scaled, chunksize)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decomposition not initialized yet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# if the input vector is in fact a corpus, return a transformed corpus as a result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: decomposition not initialized yet"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 1.573 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "2017-02-23 14:07:46,355 : DEBUG : Building prefix dict from /home/skydome20/Desktop/dbot/dbot.py3/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "2017-02-23 14:07:46,362 : DEBUG : Loading model from cache /tmp/jieba.u300776cf1c5f564361e07846b411a882.cache\n",
      "Loading model cost 2.252 seconds.\n",
      "2017-02-23 14:07:48,614 : DEBUG : Loading model cost 2.252 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-02-23 14:07:48,622 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= No filter ===============\n",
      "1st keywords:['Uber', '罰款', '議題']\n",
      "2nd keywords:The number of news is :120\n",
      "[('台灣', 55), ('uber', 53), ('議題', 46), ('司機', 42), ('服務', 31), ('司法', 19), ('計程車', 18), ('合作', 17), ('討論', 16), ('委員', 16)]\n",
      "['台灣', 'uber', '司機', '服務', '司法']\n",
      "3rd keywords:The number of news is :158\n",
      "[('台灣', 116), ('司機', 49), ('服務', 20), ('司法', 18), ('遊覽車', 16), ('日本', 15), ('小時', 13), ('計程車', 12), ('貿易', 11), ('駕駛', 10)]\n",
      "['遊覽車', '日本', '小時', '計程車', '貿易']\n",
      "4nd keywords:The number of news is :143\n",
      "[('遊覽車', 83), ('日本', 66), ('駕駛', 34), ('司機', 31), ('台灣', 30), ('貿易', 24), ('小時', 16), ('計程車', 12), ('行程', 12), ('表示', 11)]\n",
      "['駕駛', '行程', '表示']\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
